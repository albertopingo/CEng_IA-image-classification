{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# TWO FOLDERS UP\n",
    "data_dir = os.path.abspath(os.path.join(current_dir, os.pardir, os.pardir, 'data'))\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "\n",
    "train_dirs = []\n",
    "for i in range(1, 5):\n",
    "    train_dirs.append(os.path.join(train_dir, 'train' + str(i)))\n",
    "\n",
    "validation_dir = os.path.join(data_dir, 'train', 'train5')\n",
    "\n",
    "print(current_dir)\n",
    "print(data_dir)\n",
    "print(test_dir)\n",
    "print(train_dir)\n",
    "print(validation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import image_dataset_from_directory\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load training datasets from train1 to train4\n",
    "train_datasets = []\n",
    "IMG_SIZE = 32\n",
    "BATCH_SIZE = 64\n",
    "for i in range(1, 5):\n",
    "    dataset = image_dataset_from_directory(train_dirs[i-1], image_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, label_mode='categorical')\n",
    "    train_datasets.append(dataset)\n",
    " \n",
    "train_dataset = train_datasets[0]\n",
    "for dataset in train_datasets[1:]:\n",
    "    train_dataset = train_dataset.concatenate(dataset)\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(validation_dir, image_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, label_mode='categorical')\n",
    "\n",
    "\n",
    "test_dataset = image_dataset_from_directory(test_dir, image_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, label_mode='categorical')\n",
    "\n",
    "class_names = validation_dataset.class_names\n",
    "class_names = [class_name.split('_')[-1] for class_name in class_names]\n",
    "\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for data, _ in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        plt.imshow(data[i].numpy().astype('uint8'))\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers, regularizers\n",
    "\n",
    "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "x = layers.Rescaling(1./255)(inputs) # Normalize the pixel values to be between 0 and 1\n",
    "\n",
    "## First Convolutional Block\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, kernel_initializer='he_normal', activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "x = layers.BatchNormalization()(x) # Standardize the inputs to the next layer, stabilizes and speeds up training\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, kernel_initializer='he_normal', activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "# First Block - Max Pooling and Dropout\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Dropout(0.3)(x) # Drops 30% of the neurons randomly during training to prevent overfitting\n",
    "\n",
    "# Second Convolutional Block\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, kernel_initializer='he_normal', activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, kernel_initializer='he_normal', activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "# Second Block - Max Pooling and Dropout\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(512, kernel_initializer='he_normal', activation=\"relu\")(x) # Fully connected layer\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = layers.Dense(10,  kernel_initializer='glorot_uniform', activation=\"softmax\")(x)  # Softmax for multi-class classification\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_acc', \n",
    "    patience=3, \n",
    "    verbose=1, \n",
    "    factor=0.5, \n",
    "    min_lr=1e-5)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_acc', \n",
    "                           patience=4, \n",
    "                           restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('models/S01/checkpoints/S01-cp.h5', save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[early_stop, model_checkpoint, learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(model, 'models/S01/S01-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "keras.models.load_model('models/S01/S01-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(validation_dataset)\n",
    "print('val_acc:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the history from the training process\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for features, labels in validation_dataset:\n",
    "    predictions = model.predict(features)\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Load an image\n",
    "img = tf.keras.preprocessing.image.load_img(train_dirs[0] + '/006_frog/alytes_obstetricans_s_000179.png', target_size=(32, 32), interpolation='bilinear')\n",
    "# img = tf.keras.preprocessing.image.load_img(train_dirs[0] + '/000_airplane/airbus_s_000012.png', target_size=(32, 32), interpolation='bilinear')\n",
    "\n",
    "# Preprocess the image\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "print(img_array.shape)\n",
    "result = model.predict(img_array)\n",
    "print(\"Result: \", result.round())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
